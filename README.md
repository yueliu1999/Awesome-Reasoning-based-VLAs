# Awesome-Reasoning-based-VLAs



Awesome-Reasoning-based-VLAs is a collection of state-of-the-art, novel, exciting reasoning-based Vision-Language-Action models (VLAs). It contains papers, codes, datasets, evaluations, and analyses.

| Time | Title                                                      |  Venue  |                           Paper                            |                            Code                            |
| ---- | -------------------------------------------------------- | :-----: | :-------------------------------------------------------: | :-------------------------------------------------------: |
| 2025.03 | **Gemini Robotics brings AI into the physical world** |   Google     | [link]([https://arxiv.org/abs/2502.03729](https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/)) |       -    |
| 2025.02 | **Action-Free Reasoning for Policy Generalization** |   arXiv     | [link](https://arxiv.org/abs/2502.03729) |       -    |
| 2025.02 | **ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration** |   arXiv     | [link](https://arxiv.org/abs/2502.19250) |        [link](https://objectvla.github.io/)    |
| 2024.12 | **Improving Vision-Language-Action Models via Chain-of-Affordance** |   arXiv     | [link](https://arxiv.org/abs/2412.20451) |        [link](https://chain-of-affordance.github.io/)    |
| 2024.07 | **Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression** |   arXiv     | [link](https://arxiv.org/html/2412.03293v1) |        [link](https://diffusion-vla.github.io/)    |
| 2024.07 | **RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation** |   arXiv     | [link](https://arxiv.org/pdf/2406.04339) |        [link](https://sites.google.com/view/robomamba-web)    |
| 2023.12 | **Robotic Control via Embodied Chain-of-Thought Reasoning** |   arXiv     | [link](https://arxiv.org/abs/2407.08693) |        [link](https://embodied-cot.github.io/)    |
| 2023.12 | **ThinkBot: Embodied Instruction Following with Thought Chain Reasoning** |   arXiv     | [link](https://arxiv.org/abs/2312.07062) |        -   |
| 2023.07 | **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control** |   arXiv     | [link](https://arxiv.org/pdf/2503.07572) |        [link](https://robotics-transformer2.github.io/)    |








## Contributors
<a href="https://github.com/yueliu1999" target="_blank"><img src="https://avatars.githubusercontent.com/u/41297969?s=64&v=4" alt="yueliu1999" width="72" height="72"/></a> 

























