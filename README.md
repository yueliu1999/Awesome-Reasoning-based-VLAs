# Awesome-Reasoning-based-VLAs



Awesome-Reasoning-based-VLAs is a collection of state-of-the-art, novel, exciting reasoning-based Vision-Language-Action models (VLAs). It contains papers, codes, datasets, evaluations, and analyses.

| Time | Title                                                      |  Venue  |                           Paper                            |                            Code                            |
| ---- | -------------------------------------------------------- | :-----: | :-------------------------------------------------------: | :-------------------------------------------------------: |
| 2025.03 | **Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks** |   arXiv'25     | [link](https://arxiv.org/abs/2503.21696) |       -    |
| 2025.03 | **Gemini Robotics brings AI into the physical world** |   Google     | [link](https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/) |       -    |
| 2025.03 | **CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models** |   CVPR'25     | [link](https://cvpr.thecvf.com/virtual/2025/poster/33233) |       -    |
| 2025.02 | **Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models** |   arXiv     | [link](https://arxiv.org/abs/2502.19417) |       -    |
| 2025.02 | **Action-Free Reasoning for Policy Generalization** |   arXiv     | [link](https://arxiv.org/abs/2502.03729) |       -    |
| 2025.02 | **ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration** |   arXiv     | [link](https://arxiv.org/abs/2502.19250) |        [link](https://objectvla.github.io/)    |
| 2024.12 | **Improving Vision-Language-Action Models via Chain-of-Affordance** |   arXiv     | [link](https://arxiv.org/abs/2412.20451) |        [link](https://chain-of-affordance.github.io/)    |
| 2024.07 | **Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression** |   arXiv     | [link](https://arxiv.org/html/2412.03293v1) |        [link](https://diffusion-vla.github.io/)    |
| 2024.07 | **RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation** |   NeurIPS'24     | [link](https://arxiv.org/pdf/2406.04339) |        [link](https://sites.google.com/view/robomamba-web)    |
| 2023.12 | **Robotic Control via Embodied Chain-of-Thought Reasoning** |   CoRL'24     | [link](https://arxiv.org/abs/2407.08693) |        [link](https://embodied-cot.github.io/)    |
| 2023.12 | **ThinkBot: Embodied Instruction Following with Thought Chain Reasoning** |   ICLR'25     | [link](https://arxiv.org/abs/2312.07062) |        -   |
| 2023.07 | **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control** |   CoRL'23     | [link](https://arxiv.org/pdf/2503.07572) |        [link](https://robotics-transformer2.github.io/)    |
| 2024.03 | **RAT: retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation** |   CoRL'23     | [link](https://arxiv.org/pdf/2403.05313) |        [link]()    |
| 2023.03 | **React: Synergizing reasoning and acting in language models** |   ICLR'23     | [link](https://par.nsf.gov/servlets/purl/10451467) |        [link]()    |
| 2023.10 | **Tree-planner: Efficient close-loop task planning with large language models** |   arXiv'23     | [link]([https://par.nsf.gov/servlets/purl/10451467](https://arxiv.org/pdf/2310.08582)) |        [link]()    |
| 2024.06 | **Openvla: An open-source vision-language-action model** |   arXiv'24     | [link]([https://arxiv.org/pdf/2406.09246)) |        [link]()    |
| 2022.10 | **Vima: General robot manipulation with multimodal prompts** |   arXiv'24     | [link]([https://authors.library.caltech.edu/records/1bseh-9e548/files/2210.03094.pdf)) |        [link]()    |
| 2023.03 | **Palm-e: An embodied multimodal language model** |   ICML'23     | [link]([https://openreview.net/pdf?id=VTpHpqM3Cf)) |        [link]()    |
| 2022.09 | **Code as policies: Language model programs for embodied control** |   ICRA'23     | [link]([https://arxiv.org/abs/2209.07753)) |        [link]()    |
| 2024.04 | **Closed-loop open-vocabulary mobile manipulation with gpt-4v** |   CoRR'24     | [link]([https://arxiv.org/pdf/2404.10220?)) |        [link]()    |
| 2023.05 | **Reasoning with language model is planning with world model** |   EMNLP'23     | [link]([https://arxiv.org/pdf/2305.14992)) |        [link]()    |
| 2024.09 | **Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation** |   CoRL'24     | [link]([https://arxiv.org/abs/2409.01652])) |        [link]()    |





**Code as reasoning (Spatial Constraints):**

CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models

Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning

Code as policies: Language model programs for embodied control

VoxPoser: Composable 3D Value Mapsfor Robotic Manipulation with Language Models

Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation

Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection


**Explicit Compact CoT**

Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models

CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models

Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks

Robotic Control via Embodied Chain-of-Thought Reasoning

ThinkBot: Embodied Instruction Following with Thought Chain Reasoning



**Implicit Latent CoT**

RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control

Openvla: An open-source vision-language-action model

Palm-e: An embodied multimodal language model



**Others**

Gemini Robotics brings AI into the physical world

Action-Free Reasoning for Policy Generalization

ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration

Improving Vision-Language-Action Models via Chain-of-Affordance

Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression

RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation

RAT: retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation

React: Synergizing reasoning and acting in language models

Tree-planner: Efficient close-loop task planning with large language models


Vima: General robot manipulation with multimodal prompts


Closed-loop open-vocabulary mobile manipulation with gpt-4v

Reasoning with language model is planning with world model




## Contributors
<a href="https://github.com/yueliu1999" target="_blank"><img src="https://avatars.githubusercontent.com/u/41297969?s=64&v=4" alt="yueliu1999" width="72" height="72"/></a> 

























